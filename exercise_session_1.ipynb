{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smart Distribution Systems (B-KUL-H00P3A) - Exercise session 1\n",
    "Prof. Geert Deconinck"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This exercise session will familiarize the students with basic concepts of machine learning by giving them a first introduction to the most used lbraries in data science."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The students will learn to:\n",
    "* set up a machine learning environment, using state-of-the-art tools, such as keras, tensorflow and theano in Python;\n",
    "* implement and train a two-layer neural network using Keras;\n",
    "* perform the initial data exploration steps of a real-life forecasting problem in power systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to import the libraries necessary during this exercise session.\n",
    "* Pandas is a library providing intuitive data structures for data analysis of labeled data, you can think about these structures (DataFrames) as tables. You can find the documentation here: http://pandas.pydata.org/pandas-docs/stable/.\n",
    "* Numpy is the fundamental package for scientific computing in Python. Simply put, Numpy provide MatLab-like functionality to Python. Numpy-documentation: https://docs.scipy.org/doc/numpy-1.14.0/reference/.\n",
    "* Matplotlib provides a MatLab-like plotting interface to python. Docs: https://matplotlib.org/api/pyplot_api.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib ipympl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'seed' of the numpy random generators determines which '(pseudo-)random-numbers' are being generated. <br>\n",
    "To have the same results between runs and between groups, we set the seed of the random generator. More info: https://stackoverflow.com/questions/21494489/what-does-numpy-random-seed0-do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Linear (ridge) regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get familiar with the basic syntax of Python and the different libraries used in this exercise session, first a really simple linear regression problem is solved. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by generating a simple dataset. <br>\n",
    "The dataset consists of (x,y) pairs. The x-values are random numbers between 0 and 100. The y-values are constructed by adding a small random number (drawn from N(0,3)) to the x-value. As a result, if we would like to predict the y-value for an unknown x-value, the best guess would be to simply predict x. <br>\n",
    "Info on the different numpy-functions used: https://docs.scipy.org/doc/numpy/reference/routines.random.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0\n",
    "b = 100\n",
    "x = np.random.random_sample(size=(50, 1)) # draw 50 random numbers from [0,1.0) return them as vector with the shape (50, 1)\n",
    "x = (b-a) * x + a # rescale to the interval [0, 100)\n",
    "mean = 0\n",
    "sigma = 3\n",
    "r =  sigma * np.random.randn(50, 1) + mean # draw a number form N(0,1) and scale it to N(mean,sigma)\n",
    "y = x + r # add the vector of 50 random numbers to the vector of x-values to get the y-values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using matplotlib, we can easily plot these points on a scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By means of illustration, we can now train a linear regression model to approximate the funcion y=f(x). <br>\n",
    "We will use ridge-regression. Therefore, we import 'linear_model' from scikit learn. Scikit-learn is a Python library build on Numpy and offers easy-to-use machine learning functions. <br>\n",
    "The documentation of scikit-learn is found here: http://scikit-learn.org/stable/. <br>\n",
    "And the documentation of linear_model.Ridge(): http://scikit-learn.org/stable/modules/linear_model.html#ridge-regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "model = linear_model.Ridge(alpha=0.5)\n",
    "model.fit(x, y) # the input-matrix should have the shape [n_samples, n_features]. Thus, in this case [50, 1]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now fitted our regression model called, using our generated samples. To see if our model approximates the function y=x, we can generate a new set of x-values and use the model to predict the y-values. <br>\n",
    "In the next cell, generate a vector called x_val (validation set) containing 20 random numbers in [0, 100]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use our linear model to predict the y-values of x_val and again make a scatter plot. The y-values should be on the line y=x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val = model.predict(x_val)\n",
    "plt.figure()\n",
    "plt.scatter(x_val, y_val)\n",
    "plt.plot(x_val, y_val, color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous regression problem, there was clear linear relation between the inputs (x-values) and the targets (y-values). This is obviously not always the case. To illustrate this point we will now to try to apprximate a sine-function with the same ridge regression method. <br>\n",
    "In the next cell, generate a vector (call it x_train) with shape (500,1) containing 500 random numbers in the interval [0, 2*Pi). Calculate the sine of all elements in this vector and call this y_train. <br>\n",
    "Hint: check the numpy documentation for sin(x)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot this training set together with the actual sine-function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate values to plot the actual sine-function\n",
    "x_sin = np.arange(0, 2*np.pi, 0.1) # a list containing all the numbers in [0, 2*pi] with a step of 0.1\n",
    "y_sin = np.sin(x_sin) # the sine of all these numbers\n",
    "# plot the sine-function as well as a scatter plot of the training set\n",
    "plt.figure()\n",
    "plt.scatter(x_train, y_train)\n",
    "plt.plot(x_sin, y_sin, color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have again a vector of inputs and targets for our linear model, we can fit it to approximate the sine function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = linear_model.Ridge(alpha = .5)\n",
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In most machine learning problems we would like to have three data-sets: training-set, validation-set and test-set. Each set has its own functionality in the process. Make sure you understand the purpose of these sets. https://stats.stackexchange.com/questions/19048/what-is-the-difference-between-test-set-and-validation-set is good starting point. <br>\n",
    "Simply put, we use the training set to train our model. The validation is not used to train the model but to evaluate the performance of a trained model. Based on this evaluation, the model architecture might be changed. Since, after tuning of the architecture, the performance on the validation set is thus probably too optimistic (why?), we use a final test-set to get an accurate estimate of the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a new vector of x-values (between 0 and 2*Pi). Generate more or less 50 test-set values. Calculate both their true values and their predictions. Draw one scatter plot including both the targets and the predictions, in red and blue respectively. <br>\n",
    "Call the x-values: 'x_test'. The true y-values and the prediction: 'y_test' and 'y_pred', respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "plt.figure()\n",
    "plt.scatter(x_test, y_pred)\n",
    "plt.scatter(x_test, y_test, color='red')\n",
    "plt.plot(x_sin, y_sin, color='green', alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why are the predictions not representing the sine-function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear that we need other regression techniques to be able to approximate a broader range of functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Non-linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since in a lot of real-life problems it is required to apprximate a non-linear function, we need methods to do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Kernel ridge-regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different techniques exist. We will first extend linear ridge-regression to a non-linear variant, using the so-called 'kernel-trick' (https://towardsdatascience.com/understanding-the-kernel-trick-e0bc6112ef78). Here, it suffices to know the basic philosophy behind kernel functions. With a kernel function we transform the input-space to a higher dimensional space. In this space, our non-linear function might be linear and we solve a linear regression problem. Afterwards, we transform our solution back to the original space. Simply put, we are thus able to approximate a non-linear function in our input-space by approximating a linear-function in a higher dimensional space. <br>\n",
    "In the next piece of code we will use the same inputs and targets (of the sine-function) but now perform ridge regression with a radial basis kernel-function (rbf). More info about radial basis functions can be found here: https://en.wikipedia.org/wiki/Radial_basis_function. <br>\n",
    "Thereafter, we use the same x_test and y_test you generated during the linear ridge regression to estimate y_pred. We plot both the linear regression solution, and new solution.Â£"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import kernel_ridge\n",
    "model = kernel_ridge.KernelRidge(alpha=3.5, kernel='rbf', gamma=0.1)\n",
    "model.fit(x_train, y_train)\n",
    "y_pred_kernel = model.predict(x_test)\n",
    "plt.figure()\n",
    "plt.subplot(211)\n",
    "plt.scatter(x_test, y_pred) # the predictions made using ridge regression\n",
    "plt.scatter(x_test, y_test, color='red') # the ground truth\n",
    "plt.plot(x_sin, y_sin, color='green', alpha=0.3) # the sine-function, per reference\n",
    "plt.subplot(212)\n",
    "plt.scatter(x_test, y_pred_kernel) # the predictions made by kernel ridge regression\n",
    "plt.scatter(x_test, y_test, color='red')\n",
    "plt.plot(x_sin, y_sin, color='green', alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do the results compare?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nowadays, neural networks (NNs) are highly used for regression purposes. Neural networks have already been explained in the introduction slides. In this exercise session we will be using Keras, a high level neural network API in Python. Keras can use different backends, here we use the 'tensorflow' backend. Keras makes dealing with NNs easy. You can find the Keras documentation here: https://keras.io/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import RMSprop, SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to read the Keras docs dealing with the Sequential model (https://keras.io/getting-started/sequential-model-guide/). The sequential model is the most fundamental part of Keras and allows to sequentially connect different layers of a NN. Thereafter, this model can be compiled and trained. All these low-level functionalities have been implemented in Keras. Next, we create a sequential model and add one hidden layer and one output layer. We use a 'sigmoid' activation function for the hidden layer, and a linear activation function for the output layer. In order to have a non-linear regression model, we need at least one non-linear activation function. Since our model only requires one output, our output layer consists of one neuron. We, arbitrarly, decide to have 12 hidden neurons. Furthermore, we are going to use the RMSprop optimizer and a mean squared error. <br>\n",
    "The amount of hidden neurons and the activation function can greatly affect the performance of our model. For now, this model suffices, but make sure to experiment with different NN architectures once you try to solve a more complex regression problem. <br>\n",
    "* More info on activation functions: https://en.wikipedia.org/wiki/Activation_function, https://keras.io/activations/ <br>\n",
    "* Good practices with regards to NN architecture: https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw <br>\n",
    "* More info on why we need a optimizer and the different optimizers available: https://towardsdatascience.com/types-of-optimization-algorithms-used-in-neural-networks-and-ways-to-optimize-gradient-95ae5d39529f, https://keras.io/optimizers/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neurons = [12, 1]\n",
    "activation_functions = ['sigmoid', 'linear']\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(neurons[0], input_dim=1, activation=activation_functions[0], kernel_initializer='random_uniform'))\n",
    "model.add(Dense(neurons[1], activation=activation_functions[1], kernel_initializer='random_uniform'))\n",
    "    \n",
    "rprop = RMSprop(lr=0.001, rho=0.9, epsilon=1e-6)\n",
    "model.compile(loss='mean_squared_error', optimizer=rprop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now fit this model to our earlier defined training set (x_train, y_train). <br> \n",
    "Running the next piece of code, can take a while (wait until the MSE is printed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_training = model.fit(x_train, y_train, epochs=700, batch_size=6, verbose=0) # train the model\n",
    "mse = output_training.history['loss'][-1] # from the 'history' of the error during training, take the last element (-1)\n",
    "print('- mse is %.4f' % mse + ' @ ' + str(len(output_training.history['loss']))) # print the final MSE and at which episode it occured"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we can plot the predictions and the actual y_test values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the test set y-values\n",
    "y_pred = model.predict(x_test)\n",
    "plt.figure()\n",
    "plt.scatter(x_test, y_pred, alpha=0.7) # plot the y-values predicted by the NN\n",
    "plt.scatter(x_test, y_test, color='red', alpha=0.4) # plot the actual y-values, a bit transparant\n",
    "plt.plot(x_sin, y_sin, color='green', alpha=0.3) # plot the sine-function\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot gives a pretty good idea about the performance of the NN. <br>\n",
    "Feel free to experiment with different NN architectures, optimization algorithms and activation functions, by re-running the past 3 cells. <br>\n",
    "A clear performance measure can give a better indication than a graph. Implement the mean_squared_error function in the next cell. This function should return the mean squared error between the vectors x and y. You can assume the vector have the shape (n_samples, n_features). <br>\n",
    "Check the numpy docs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this function to calculate the MSE between y_test and y_pred. <br>\n",
    "Print it, and check wether it deviates from the error on the training set (printed after training). Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, in real life, datapoints will almost never exactly match the underlying function, and there will always be a bit of noise in the measurements, e.g. due to noise of a sensor. <br>\n",
    "Lets make it a bit more challenging for the NN and add noise to the y_train values. By doing this the tuples (x,y) won't exactly match the sine-function anymore. We can show this by plotting the sine and the training set. <br>\n",
    "You can play with the value of the standard deviation (sigma) and see the result on the accuracy of the prediction (of the test-set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = 0.1\n",
    "y_train_noise = y_train + sigma * np.random.randn(y_train.shape[0], y_train.shape[1])\n",
    "plt.figure()\n",
    "plt.plot(x_sin, y_sin, color='red')\n",
    "plt.scatter(x_train, y_train_noise, alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To have a fair comparison we need to re-initialize the weights of the model. Otherwise the results will be too optimistic since the NN is already trained on a sine-function. Also, RMSprop keeps an internal state because it has some variables that change based on the training progess. Therefore, we'll recompile the model as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_weights(model):\n",
    "    session = K.get_session()\n",
    "    for layer in model.layers: \n",
    "        if hasattr(layer, 'kernel_initializer'):\n",
    "            layer.kernel.initializer.run(session=session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_weights(model)\n",
    "model.compile(loss='mean_squared_error', optimizer=rprop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, fit the NN to the new (!) training set and calculate the MSE on the test set. Again plot the test set ground truth and predictions. Add the actual sine as a reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you think, is the NN handling the noise well?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks have been know to perform supervised learning (https://en.wikipedia.org/wiki/Supervised_learning) pretty well, as long as they have enough examples. <br>\n",
    "Next, select 10 of the points from the training set (x_train, y_train_noise). Make sure to select tuples, and not x and y independently. Run the previous cell again, with this new (smaller) training set. <br>\n",
    "Is the result as good/bad as you expected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(x_train, y_train_noise)\n",
    "plt.plot(x_sin, y_sin, color='green')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the problems of Neural Networks (actually, regression in general) is the problem of overfitting. Especially when there aren't many training samples available, it is possible to 'specialize' the NN too much to these available samples. The predictions will then correspond too closely to these samples and won't generalize as good anymore to unseen samples. <br>\n",
    "More info on overfitting here: https://datascience.stackexchange.com/questions/61/why-is-overfitting-bad-in-machine-learning. <br>\n",
    "An important principle to take into account when deciding an architecture of a NN is Occams Razor (https://en.wikipedia.org/wiki/Occam%27s_razor). This principles states that (not only with NNs) one should select a model with the fewest assumptions. In the case of NN, this means the 'simplest' architecture. <br>\n",
    "In what follows we will expereimentaly show the problem of overfitting, based on the running sine-function example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will create a new training-, validation- and test-set. We will add a bit of noise to the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed=54) # set a new seed, such that the results of everybody are similar\n",
    "sigma = 0.15\n",
    "train_size = 30 \n",
    "val_size = 10\n",
    "x_train = np.arange(0, 2*np.pi, 0.1).reshape(-1, 1)\n",
    "y_train = np.sin(x_train) + sigma * np.random.randn(x_train.shape[0], 1)\n",
    "x_val = 2*np.pi * np.random.random_sample((val_size, 1))\n",
    "y_val = np.sin(x_val)\n",
    "# x_test = 2*np.pi * np.random.random_sample((1, 70)).reshape((-1, 1))\n",
    "# y_test = np.sin(x_test)\n",
    "x_test = x_sin.reshape(-1, 1)\n",
    "y_test = y_sin.reshape(-1, 1)\n",
    "plt.figure()\n",
    "plt.scatter(x_train, y_train)\n",
    "plt.scatter(x_val, y_val, color='brown')\n",
    "plt.plot(x_sin, y_sin, color='green', alpha=0.3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now train the previous quite simple model on this highly corrupted training set and pot the predictions on the test-set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neurons = [12, 1]\n",
    "activation_functions = ['sigmoid', 'linear']\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(neurons[0], input_dim=1, activation=activation_functions[0], kernel_initializer='random_uniform'))\n",
    "model.add(Dense(neurons[1], activation=activation_functions[1], kernel_initializer='random_uniform'))\n",
    "\n",
    "optim = RMSprop(lr=0.01, rho=0.9, epsilon=1e-6)\n",
    "model.compile(loss='mean_squared_error', optimizer=optim)\n",
    "\n",
    "output_training = model.fit(x_train, y_train, epochs=3000, batch_size=6, verbose=0) # train the model\n",
    "mse = output_training.history['loss'][-1] # from the 'history' of the error during training, take the last element (-1)\n",
    "print('- mse is %.4f' % mse + ' @ ' + str(len(output_training.history['loss']))) # print the final MSE and at which episode it occured\n",
    "y_pred = model.predict(x_test)\n",
    "plt.figure()\n",
    "plt.plot(x_test, y_pred) # plot the y-values predicted by the NN\n",
    "plt.plot(x_sin, y_sin, color='green', alpha=0.6) # plot the sine-function\n",
    "plt.show()\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything went well, you can see the result isn't really good, but still the sine-shape is clear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets now train a more elaborate model on the same training set, and check it's performance on the same test-set. This time we will also perform a lot more episodes (training might take quite long). <br>\n",
    "In the next cell, create a model with 2 hidden layers, both should have 30 neurons and the 'sigmoid' activation layer. Compile the model and use RMSprop (with the same parameter values, to have a fair comparison) again as the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save the initial weights, such that we can start "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ww = model.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train this model for a lot (7000-10000) episodes, with a batch_size of 1. After training, we plot the results on the test-set again. Compare this with the result of the smaller NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_training = model.fit(x_train, y_train, epochs=8000, batch_size=1, verbose=0, validation_data=(x_val,y_val)) # train the model\n",
    "mse = output_training.history['loss'][-1] # from the 'history' of the error during training, take the last element (-1)\n",
    "print('- mse is %.4f' % mse + ' @ ' + str(len(output_training.history['loss']))) # print the final MSE and at which episode it occured\n",
    "y_pred = model.predict(x_test)\n",
    "plt.figure()\n",
    "plt.plot(x_test, y_pred) # plot the y-values predicted by the NN\n",
    "plt.plot(x_sin, y_sin, color='green', alpha=0.3) # plot the sine-function\n",
    "plt.show()\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print('test-set mse' + str(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this result it is clear the model is overfitting on the noise in the training data. We can visualize this result by plotting both the error on the training set and the validation set in function of the episodes performed. <br>\n",
    "Remember that the validation set is not used during training itself. <br>\n",
    "For visualization purposes we plot a running average of the error on both sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / float(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_window=30\n",
    "plt.figure()\n",
    "train_loss = plt.plot(running_mean(x=output_training.history['loss'], N=mean_window), label='training error')\n",
    "val_loss = plt.plot(running_mean(x=output_training.history['val_loss'], N=mean_window), color='red', label='validation error')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this figure you should see that, while the training set error keeps decreasing (why?), the validation-set error first decreases but starts increasing after a lot of episodes. This shows overfitting occurs. Make sure you understand why this is happening. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find more info on overfitting and possible ways to avoid it here: https://nl.mathworks.com/help/nnet/ug/improve-neural-network-generalization-and-avoid-overfitting.html. <br>\n",
    "Here, we will use Early Stopping. This is a quite simple way to avoid overfitting. It stops the training process when the validation-set error starts increasing (https://en.wikipedia.org/wiki/Early_stopping). <br>\n",
    "Keras uses 'callbacks' which can be given as an argument in the fit() function. The documentation of the EarlyStopping callback in keras: https://keras.io/callbacks/#earlystopping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, use the same elaborate model and train and evaluate it again on the same sets. Use again a lot of episodes, but make sure you append the early stopping callback to the fit() method. Choose a reasonable value for the 'patience' parameter (based on the previous result). Plot the result, and the errors in function of the episodes. Evaluate the differences between the previous result. Which of both results would you prefer in a real-life regression problem? Why? <br>\n",
    "Initialize the model with the same weights it had initially (for the previous experiment) also don't forget to recompile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(running_mean(x=output_training.history['loss'], N=mean_window), label='training error')\n",
    "plt.plot(running_mean(x=output_training.history['val_loss'], N=mean_window), color='red', label='validation error')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Early stopping is one of the reasons why we can't use the validation set in our estimate of the final model's performance, because we have made important design decision based on the validation set. <br>\n",
    "But remember that eventhough early stopping might avoid overfitting, we should still take into account Occams Razor. Large networks are computionally more expensive and thus we prefer to take the 'simplest' model which still works well for our problem. Unfotunately, there is no standard way to find out what this model is. You can find some guidelines in the earlier mentioned link: https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw. <br>\n",
    "Furthermore, as mentioned earlier, we can use the validation set to tune the architecture of the model. Mostly, this is done through a search through the so-called hyperparameter space. Important hyperparameters include: amount of hidden layers, amount of neurons in each layer, activation function, ... . <br>\n",
    "We can thus start from the mentioned guidelines and create a model for a couple of possible combinations of these hyperparameters. All these models can then be evaluated based on the validation set. Afterwards, we can choose the best performing model. The scikit-learn library of Python provides this functionality. Read more about it here: http://scikit-learn.org/stable/modules/grid_search.html. It is also possible to avoid the need of a validation set for this purpose. In cross-calidation the training set is split in N parts, N-1 parts are then used to train the model and 1 part is used as a validation set. This is then done several times, to get a more reliable estimate. More info on cross-validation: http://scikit-learn.org/stable/modules/cross_validation.html, https://stats.stackexchange.com/questions/301462/what-is-the-purpose-of-crossvalidation. <br>\n",
    "Keras has a scikit learn API to be able to use your models within scikit-learn functions: https://keras.io/scikit-learn-api/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
